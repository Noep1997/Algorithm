# -*- coding: utf-8 -*-
"""CS181 Practical #1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1y1vSeXuL8h7gbMPDLVGPa6aSz3dDgoKA

## CS181 - Practical #1 - Philippe Noël, Jorma Görns, Arjun Verma

Some of the code from this practical is inspired by the sample.ipynb notebook provided by the course staff.
"""

# Import libraries
import matplotlib
import numpy as np
import scipy as sp
import pandas as pd
import matplotlib.cm as cmx
import matplotlib.pyplot as plt
import matplotlib.colors as colors
from collections import Counter
from google.colab import files

# models & metrics
from sklearn.neural_network import MLPRegressor
from sklearn.linear_model import LinearRegression as Lin_Reg
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error

# %matplotlib inline

# Load training and test sets (assumes you have these in current working directory)
train = pd.read_csv("train.csv")
test = pd.read_csv("test.csv")

# Inspect training set
train.head()

# Inspect test set
test.head()

# Explore distribution of target
plt.hist(train['Target'], bins = 100)
plt.title("Distribution of Dependent Variable")
plt.show()

# Split training set into X and y (removing first column containing IDs)
X_train = train.iloc[:, 1:-1]
y_train = train.iloc[:, -1]

# Split test set into X and y (removing first column containing IDs)
X_test = test.iloc[:, 1:]

# Define function to compute RMSE
def scoreRMSE(predictor, X, true_y):
    predictions = predictor.predict(X)    
    return np.sqrt(mean_squared_error(predictions, true_y))

# Fit unregularized linear regression and see RMSE on training set
linReg = Lin_Reg()
linReg.fit(X_train, y_train)

print("Training RMSE: ", scoreRMSE(linReg, X_train, y_train))

"""So far we have manipulated, imported the data and ran a simple unregularized linear regression on it. This is the benchmark to beat. We will try a few models."""

# Exploring the data by plotting each of the features against the value that 
# we're trying to predict: Target
for feature in train.columns:
    plt.scatter(train[feature], train['Target'])
    plt.title(feature + ' vs ' + 'Target')
    plt.xlabel(feature)
    plt.ylabel('Target')
    plt.show()

"""As we can see, a lot of features are full on zero (or a fixed value no matter what y is). We could trim those features away to reduce the dimensionality of our data set using PCA. We will do that further down if necessary to improve the model proves to not perform well enough.

The first model we are going to test is Ridge Regression, since we have seen that adding regularization to linear regression can make for a reasonably strong model. We're unsure how this is going to perform on so many features, but it seem to us like a good first start based on what we have done thus far in the class. We use a Ridge Regression model with cross-validation built in to reduce overfitting.
"""

from sklearn.linear_model import RidgeCV

# fit a Ridge regression model with cross-validation to the training set
model_ridgecv = RidgeCV()
model_ridgecv.fit(X_train, y_train)

print("Ridge with Cross-Validation Training RMSE: ", scoreRMSE(model_ridgecv, X_train, y_train))

"""As we can see, the validation score for the Ridge Regression with cross-validation is quite similar to baseline. This makes sense when thinking about it, since Ridge Regression still implements a linear relationship, which is unlikely to be representive of the actual data set here (due to the number of features). The cross-validation helps prevent overfitting, but here it is more likely that the linear model is underfitting, and thus the cross-validation has little impact. We will test different models to try to solve this underfitting problem. We try more complex models.

We first split our training data into training and validation sets, since the more complex models will not have built-in cross-validation.
"""

from sklearn.model_selection import train_test_split

# randomly split the dataset into a training and validation set
X_train_split, X_val, y_train_split, y_val = data_split = train_test_split(
    X_train, y_train, test_size=0.2, random_state=181)

"""The next model we try is a Support Vector Machine, as proposed by the practical pdf. We first attempt it with random parameters."""

from sklearn.svm import SVR

# random parameters SVM regression
model_svm = SVR(C=1.0, epsilon=0.1, gamma='scale')
model_svm.fit(X_train, y_train)

print("SVM Training RMSE: ", scoreRMSE(model_rf, X_train, y_train))
print("SVM Validation RMSE: ", scoreRMSE(model_rf, X_val, y_val))

"""This is already much better! We now try grid searching to find the optimal C and epsilon parameters for the SVM."""

# we save the best parameters here
best_C = None
best_epsilon = None

# this is about the score from our random parameters, it will be our benchmark
# to improve with grid search
best_rmse = 0.0231

# range 0.1 to 1 for each parameter here
for C in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]: 
  for epsilon in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]:  
      # setting up the model parameters
      best_model_svm = SVR(C=C, epsilon=epsilon, gamma='scale')
      best_model_svm.fit(X_train, y_train)
      
      # testing errors
      val_rmse = scoreRMSE(best_model_svm, X_val, y_val)
        
      # updating the best parameters if the avg validation accuracy is better
      if val_rmse < best_rmse:
          best_rmse = val_rmse
          best_C = C
          best_epsilon = epsilon
            
# display the best parameters and the associated RMSE
print('Best model C:', C) 
print('Best model Epsilon:', epsilon) 
print('Best validation RMSE:', best_rmse)

"""As we can see, the SVM does not improve beyond our guess, which suggests that there is still improvement. It is nonetheless a better model than Ridge Regression. We will try different, more complex models. The fact that the SVM model seems to point us in the direction that a more complex model is better to fit our data, but the fact that it does not improve indicates that it is probably not complex enough.

The next model we will try is probably the most complex we can get: a neural network. We have little experience with neural networks, but we nonetheless want to try fitting at least a random parameters neural network to see how the performance would compare.
"""

from sklearn.neural_network import MLPRegressor

# random parameters Neural Network regression
# this network does RELU activation & Adam optimizer by default
model_nn = MLPRegressor(hidden_layer_sizes=(150, 120, 80, 50, 30), max_iter=500)
model_nn.fit(X_train, y_train)

print("Neural Network Training RMSE: ", scoreRMSE(model_nn, X_train, y_train))
print("Neural Network Validation RMSE: ", scoreRMSE(model_nn, X_val, y_val))

"""As we can see, the neural network does better than Ridge Regression, but worse than the SVM before finding the optimal parameters. It seems like a neural network has the potential for being complex enough to fit our data, but the task of finding the appropriate architecture is not very straightforward to solve using grid search approach. Therefore, we will explore other complex models that are simpler to optimize and will come back to a neural network if we cannot get satisfying fit from simpler models.

The next model we will look at is a Random Forest. We first try a random parameters Random Forest.
"""

# Random forest set-up and training on random parameters guess
model_rf = RandomForestRegressor(max_depth=10, random_state=0, n_estimators=10)
model_rf.fit(X_train, y_train)

print("Random Forest Training RMSE: ", scoreRMSE(model_rf, X_train, y_train))
print("Random Forest Validation RMSE: ", scoreRMSE(model_rf, X_val, y_val))

"""This is already a huge improvement from our previous attempts! This is slightly better than all of our previous models. We will now perform grid search to optimize the parameters and try to fit the best possible random forest and improve the score even more."""

# we save the best parameters here
best_model_parameters = None

# this is about the score from our random parameters, it will be our benchmark
# to improve with grid search
best_rmse = 0.02306

# range 1 to 260 by steps of 20 for each parameter
for depth in [1, 20, 40, 60]: 
  for n_estimators in [1, 20, 40, 60, 80, 100, 120, 140, 160, 180, 200, 220]:  
        # setting up the model parameters dict
        model_parameters = {'max_depth': depth, 'random_state':0, 'n_estimators': n_estimators}
        optimized_model_rf = RandomForestRegressor(**model_parameters)
        
        # running cross validation for the model with the parameters
        optimized_model_rf.fit(X_train, y_train)
        val_rmse = scoreRMSE(optimized_model_rf, X_val, y_val)
        
        # updating the best parameters if the avg validation accuracy is better
        if val_rmse < best_rmse:
            best_rmse = val_rmse
            best_model_parameters = model_parameters
            
# display the best parameters and the associated RMSE
print('Best model parameters:', best_model_parameters) 
print('Best validation RMSE:', best_rmse)

"""This is huge! This is the best RMSE validation score we have had by far, so this is our best model. Therefore, we will do our prediction for the test set based on this model."""

# we use the model with the best parameters we found
final_opt_model_rf = RandomForestRegressor(max_depth=60, random_state=0, n_estimators=200)
final_opt_model_rf.fit(X_train, y_train)

print("Random Forest Training RMSE: ", scoreRMSE(final_opt_model_rf, X_train, y_train))
print("Random Forest Validation RMSE: ", scoreRMSE(final_opt_model_rf, X_val, y_val))

"""Perfect, we can now use this model to do the predictions. We could have gone beyond and searched for higher paramenters, but it seems a bit unnecessary, especially considering that the optimal parameters are in the middle of our range of parameter values. Seeing as this is the best we have achieved, we will use this model for our predictions."""

# make predictions using optimized Random Forest model fitted above
predictions_rf = final_opt_model_rf.predict(X_test)

"""We finally convert our predictions to a dataframe with the right submission format and inspect to make sure it is correct."""

# format predictions to be compatible with Kaggle upload
sample_submission = pd.DataFrame(data=predictions_rf, columns=['Predicted'])
sample_submission.insert(0, "Id", range(1, 1 + X_test.shape[0]))
sample_submission['Id'] = sample_submission['Id'].astype(str)
sample_submission.head()

"""We can finally convert to CSV and submit to Kaggle."""

# save predictions to .csv file for upload to Kaggle
sample_submission.to_csv("sample_submission.csv", index=False)
files.download('sample_submission.csv')

"""This is a huge improvement! This is very positive results and the best we can do with the current grid search method. We could have gone beyond and searched for higher paramenters, but it seems a bit unnecessary, especially considering that the optimal parameters are in the middle of our range of parameter values. Seeing as this is the best we have achieved, we will use this model for our predictions."""

# save predictions to .csv file for upload to Kaggle
sample_submission.to_csv("sample_submission.csv", index=False)
files.download('sample_submission.csv')

"""These results make total sense. A random forest is a complex model using ensembling of decision trees, which enables it to fit a very large multi-features model. Still, it is simple enough to be well optimized by grid searching, as we have shown. It is therefore a very good model for the task we are tasked of doing here, and can be seen by the performances.

At the writing of this file (Thursday, February 14, 22:00), this gives us a ranking of position 13 on the Kaggle competition.
"""