{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A light introduction to natural language processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In lab 3, we're going to briefly cover some methods for analyzing data that comes in the form of text. This will help with the practical next week."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Classification in Movie Reviews\n",
    "\n",
    "We'll use the dataset from Stanford here: http://ai.stanford.edu/~amaas/data/sentiment/\n",
    "\n",
    "1) First click on the link and download the dataset (It's too big to put on github)\n",
    "\n",
    "2) Make sure you move the directory \"aclImdb\" into the same folder as this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a preview I wrote a summary of the performance of a few classifiers:\n",
    "**Summary of performance of different classifiers (Accuracy)**:\n",
    "- Naive Bayes: 0.83\n",
    "- Random Forest: 0.84\n",
    "- Convolutional Neural Network: 0.85"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, the data comes in separate files, which is kind of annoying. I used glob for this. glob(\"directory/*\") just lists the filenames in that directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "#glob lets us quickly access all the filenames, either pip install it or find a different way to do this\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_filenames = glob('aclImdb/train/pos/*')\n",
    "neg_filenames = glob('aclImdb/train/neg/*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check now that pos_filenames has all the filenames for positive reviews and neg_filenames has all the filenames for negative reviews. The following code is pretty hacky, but it does the job for combining all the text into one dataframe. We'll just open the files one by one in a list and append each string to a list. We'll also keep track of the sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop through the list of files and append the contents to a list\n",
    "contents = []\n",
    "sentiments = []\n",
    "\n",
    "#loop through the positive sentiment files and save all the contents\n",
    "for fname in pos_filenames:\n",
    "    with open(fname,'rb') as f:\n",
    "        contents.append(str(f.readlines()[0]))\n",
    "        sentiments.append(1)\n",
    "        \n",
    "for fname in neg_filenames:\n",
    "    with open(fname,'rb') as f:\n",
    "        contents.append(str(f.readlines()[0]))\n",
    "        sentiments.append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the length of the list we just made (total number of movie revieews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the first movie review "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'For a movie that gets no respect there sure are a lot of memorable quotes listed for this gem. Imagine a movie where Joe Piscopo is actually funny! Maureen Stapleton is a scene stealer. The Moroni character is an absolute scream. Watch for Alan \"The Skipper\" Hale jr. as a police Sgt.'\n"
     ]
    }
   ],
   "source": [
    "print(contents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get back to familiar territory, we'll turn this into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can turn this into a pd Dataframe\n",
    "df = pd.DataFrame()\n",
    "df['txt'] = contents\n",
    "df['sentiment'] = sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>txt</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b'For a movie that gets no respect there sure ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b'Bizarre horror movie filled with famous face...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b'A solid, if unremarkable film. Matthau, as E...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b'It\\'s a strange feeling to sit alone in a th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b\"You probably all already know this by now, b...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 txt  sentiment\n",
       "0  b'For a movie that gets no respect there sure ...          1\n",
       "1  b'Bizarre horror movie filled with famous face...          1\n",
       "2  b'A solid, if unremarkable film. Matthau, as E...          1\n",
       "3  b'It\\'s a strange feeling to sit alone in a th...          1\n",
       "4  b\"You probably all already know this by now, b...          1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, cool. But we still don't really know how to deal with this. Computers aren't inherently able to understand text, so we'll need to get the \"txt\" column into a form we know how to work with in order to make predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using sklearn\n",
    "\n",
    "sklearn isn't really the best library for working with text data, so we'll keep this section relatively short. For most purposes you'll want to use NLTK or spacy. But since you're familiar with sklearn we'll start here. \n",
    "\n",
    "The main thing that we'll be using from sklearn is CountVectorizer. This will take a corpus of text and turn each document into a \"count vector.\" This count vector is essentially a histogram over the entire vocabulary (all words in the training set). As an example, consider the (fake) sentence \"dog cat cat cat bear\". Our vocab size is 3, so the sentence is represented by the three dimensional vector:\n",
    "\n",
    "$$[1,3,1] $$\n",
    "\n",
    "This is also called the bag of words representation. \n",
    "\n",
    "**Exercise 1:** what are the pros and cons of using this? Can you think of an alternative way of representing text at the sentence level? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pros: Very concise, keeps track of the frequency of each word.\n",
    "# Cons: We need another vector to store what word each number represents, lose track of order of words.\n",
    "# Alternate Way: We could use tuples, we could use a dictionary (imo the best solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sklearn has a built in count vectorizer\n",
    "- Fit: build vocabulary on some iterable containing strings\n",
    "- transform: use existing vocabulary to transform the input into a N x V sparse matrix\n",
    "- fit_transform: fit on this data, and also transform it (same as calling fit then transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 3]], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer()\n",
    "\n",
    "#by default countvectorizer will return a sparse array which is a special datatype for \n",
    "#arrays that are mostly 0s (to save space), but .toarray() will convert this back to a\n",
    "#regular numpy array\n",
    "cv.fit_transform([\"cat dog dog dog cow\"]).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As another example consider the sentence \"dog dog dog snail snail\". Since \"snail\" is not in the original vocab that we fit count_vectorizer with, it won't be part of the vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 3]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.transform([\"dog dog dog snail snail\"]).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2**: Use CountVectorizer on the Stanford dataset. This will take a few seconds. You'll want to use max_features to limit the number of words that you consider, since rare words won't help you much.  \n",
    "\n",
    "Limit the number of features to 10,000\n",
    "\n",
    "Save the result as new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 1, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 1, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# counting word counts for each movie review\n",
    "new_data = CountVectorizer(max_features=10000)\n",
    "new_data = new_data.fit_transform(df['txt']).toarray()\n",
    "display(new_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigrams vs Bigrams vs N-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we explored above, using this \"Bag of Words\" representation throws away a lot of information in the sentence. One way of trying to preserve local information is to use bigrams. This is just expanding our vocabulary to include consecutive word-pairs of length 2. So in our example: \"cat dog dog dog cow\", we would have the vocabulary\n",
    "- cat x1 \n",
    "- dog x3\n",
    "- cow x1\n",
    "- cat dog x1\n",
    "- dog dog x2 \n",
    "- dog cow x1\n",
    "\n",
    "N-grams is the extension of this to word sequences of length N. For CountVectorizer, we specify this with:\n",
    "\n",
    "ngram_range = (1, N) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 3, 1, 2]], dtype=int64)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer(ngram_range = (1,2))\n",
    "cv.fit_transform([\"cat dog dog dog cow\"]).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a simple baseline model\n",
    "\n",
    "For text classification, simple models trained on a large amount of data perform quite well. A pretty standard baseline is the Naive Bayes Model. We'll go through some of the math here. If you're not interested in it, you can skip it.\n",
    "\n",
    "Suppose that $y_i$ is your class label (in this case $y_i$ is either 0 or 1 for negative and positive). $i$ just indexes what datapoint you're looking at. We'll say that $i$ ranges from $1$ to $N$ (in other words you have $N$ sentences in your training dataset).\n",
    "\n",
    "Also we have $\\mathbf{x}_i$ which is the sentence corresponding to the label $y_i$. We use bold to denote the fact that $\\mathbf{x}_i$ is a vector where each element is a word.\n",
    "\n",
    "Naive Bayes models the joint probability density, $p(y_i, \\mathbf{x}_i)$. We do this by parameterizing the prior probability of having a certain class, and then parameterizing the probability of generating a certain sentence given that class. Using Bayes rule, we can write this as:\n",
    "\n",
    "$$p(y_i, \\mathbf{x}_i) = p(\\mathbf{x}_i | y_i) p(y_i) = p(x_{i1},\\dots,x_{iT_i} | y_i) p(y_i) $$\n",
    "\n",
    "where $x_{it}$ is the word at position $t$, and $T_i$ is the length of sentence $i$. Now we apply a huge assumption (which seems like it is just wrong, but works decently in practice). That is, we assume that $x_{i1},\\dots,x_{iT_i}$ are conditionally independent given the class, $y_i$. This lets us factor the probability as:\n",
    "\n",
    "$$p(y_i, \\mathbf{x}_i) = p(y_i)\\prod_{t=1}^T p(x_{it}| y_i)  $$ \n",
    "\n",
    "We parameterize $p(x_{it}|y_i)$ as a Multinoulli random variable, i.e.:\n",
    "\n",
    "$$p(x_{it} = dog | y_i = 0) =  \\pi_{0,dog} $$ \n",
    "\n",
    "Where $\\pi_{0,dog}$ is the probability that \"dog\" is generated given that we're in class 0 (negative). So we need 2*Vocab_size parameters for this, since we need a probability for every class for every word. For the english language, that's approximately 20,000 parameters. Also, we parameterize the prior probabilites as bernoulli random variables. That's only one extra parameter:\n",
    "\n",
    "$$p(y_i = 0) = \\theta_0$$\n",
    "$$p(y_i = 1) = 1 - \\theta_0$$\n",
    "\n",
    "Given this model, it's pretty straightforward to get a maximum likelihood estimate for all the parameters, i.e. the $\\theta$s and $\\pi$s. If you're not familiar with maximum likelihood, it just means that we choose $\\theta$ and $\\pi$s to be the values that make the observed data have the highest likelihood. This turns the learning procedure into a simple optimization problem. \n",
    "\n",
    "If we go through all the math to solve this, it actually turns out that we get:\n",
    "optimal $\\theta_0$ is the proportion of sentences that are in class $0$, the negative class, and that $\\pi_{0,dog}$ is just the proportion of words in class $0$ that are the word \"dog\". Likewise, $\\pi_{1,dog}$ is just the proportion of words in class $1$ that are the word \"dog\". \n",
    "\n",
    "So \"training\" the model is just learning these parameters through an optimization procedure. But given a sentence, how do we make a prediction of whether its positive of negative?\n",
    "\n",
    "We can express that as:\n",
    "\n",
    "$$p(y_i = 0 | \\mathbf{x}_i ) \\propto p(\\mathbf{x}_i|y_i = 0) p(y_i = 0) $$\n",
    "$$p(y_i = 1 | \\mathbf{x}_i ) \\propto p(\\mathbf{x}_i|y_i = 1) p(y_i = 1) $$\n",
    "\n",
    "To predict we just take the higher of these values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optional (hard) exercise:** implement Naive Bayes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In sklearn, this is easy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can fit the model with the normal sklearn syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.fit(new_data, df['sentiment']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code splits into train and test for evaluation of the model. We'll make a random permutation of indices and then use that to randomly shuffle our data. We'll take a 70:30 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.random.permutation makes a random permutation of indices {1...N} \n",
    "perm = np.random.permutation(range(len(df.sentiment)))\n",
    "\n",
    "#split this permutation by a 70:30 split\n",
    "trn = perm[:int(0.7*len(perm))]\n",
    "tst = perm[int(0.7*len(perm)):]\n",
    "\n",
    "#slice the processed data into train and test sets; alternatively we could have done this with \n",
    "#sklearn functions\n",
    "x_train = new_data[trn]\n",
    "x_tst = new_data[tst]\n",
    "y_train = df.sentiment[trn]\n",
    "y_tst = df.sentiment[tst]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the shape of our splits. The test set has 7500 data points and the trianing set has 17500. We have 10,000 features since there are 10,000 words in our vocab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7500,)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tst.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17500,)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17500, 10000)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7500, 10000)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_tst.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting the Naive Bayes model on our training x and y values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb = MultinomialNB()\n",
    "nb.fit(x_train, y_train) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score will give us our classification accuracy on test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.84"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.score(x_tst,y_tst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's pretty good for such a simple model. Obviously, we'll do a bit better on the data that we trained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8646857142857143"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.score(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3:** Improve the score somehow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8422857142857143"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# slight improvement by generating a better split that reduces both datasets variances\n",
    "#split this permutation by a 65:35 split\n",
    "trn = perm[:int(0.65*len(perm))]\n",
    "tst = perm[int(0.65*len(perm)):]\n",
    "\n",
    "#slice the processed data into train and test sets; alternatively we could have done this with \n",
    "#sklearn functions\n",
    "x_train = new_data[trn]\n",
    "x_tst = new_data[tst]\n",
    "y_train = df.sentiment[trn]\n",
    "y_tst = df.sentiment[tst]\n",
    "\n",
    "# fitting and scoring\n",
    "nb = MultinomialNB()\n",
    "nb.fit(x_train, y_train) \n",
    "nb.score(x_tst,y_tst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) building a more powerful model using pytorch\n",
    "\n",
    "The following is for people who have a good background in machine learning and are interested in learning about how to build neural networks to solve this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using torchtext (preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to save out tabular dataset from above as a csv and then load it with torchtext because I couldn't think of a better way to do this off the top of my head. Torchtext is a library for loading/dealing with datasets for pytorch. And pytorch is a library for implementing neural networks (similar to tensorflow). \n",
    "\n",
    "This is a lot of extra effort but hopefully buys us a couple percentage points of accuracy.\n",
    "\n",
    "We're going to use 3 new libraries. In brief, Torch is a library for building neural network architectures, torchtext is a library for preprocessing text for Torch, and tqdm lets us display progress bars to keep track of progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /anaconda/lib/python3.6/site-packages (0.4.0)\n",
      "Requirement already satisfied: torchtext in /anaconda/lib/python3.6/site-packages (0.2.3)\n",
      "Requirement already satisfied: tqdm in /anaconda/lib/python3.6/site-packages (from torchtext) (4.23.4)\n",
      "Requirement already satisfied: requests in /anaconda/lib/python3.6/site-packages (from torchtext) (2.14.2)\n",
      "Requirement already satisfied: tqdm in /anaconda/lib/python3.6/site-packages (4.23.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "!pip install torchtext\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the stuff we'll need from these libraries\n",
    "import torchtext\n",
    "from torchtext.vocab import Vectors, GloVe\n",
    "import torchtext.datasets as datasets\n",
    "\n",
    "#we'll save our data from before in a tabular dataset in two parts; train and test\n",
    "df.iloc[trn,:].to_csv('saved_dataset_train.csv',index = False,header = False)\n",
    "df.iloc[tst,:].to_csv('saved_dataset_test.csv',index = False,header = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by initializing two torchtext Field objects, which will hold label and text vocabularies. We'll load in the two datasets using these fields. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we'll initialize two fields. These will hold our vocabularies for text and our vocabulary (just postive and negative)\n",
    "# for labels\n",
    "\n",
    "TEXT = torchtext.data.Field()\n",
    "LABEL = torchtext.data.Field(sequential = False,unk_token = None)\n",
    "\n",
    "#these two lines will load in the datasets that we saved\n",
    "\n",
    "pos_train = torchtext.data.TabularDataset(path='saved_dataset_train.csv', format='csv',fields=[('txt', TEXT),\n",
    " ('sentiment', LABEL)])\n",
    "\n",
    "pos_test = torchtext.data.TabularDataset(path='saved_dataset_test.csv', format='csv',fields=[('txt', TEXT),\n",
    " ('sentiment', LABEL)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the vocabulary using the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(TEXT.vocab) 224441\n",
      "len(LABEL.vocab) 2\n"
     ]
    }
   ],
   "source": [
    "TEXT.build_vocab(pos_train)\n",
    "LABEL.build_vocab(pos_train)\n",
    "print('len(TEXT.vocab)', len(TEXT.vocab))\n",
    "print('len(LABEL.vocab)', len(LABEL.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LABEL.vocab.itos is a list that contains the vocabulary for the labels (0 and 1). TEXT.vocab.itos would give us a list with the vocabulary for all the text in our corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', '0']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LABEL.vocab.itos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're also going to use an iterator to loop through the data. We'll use torchtext's BucketIterator with a batch size of 10 (we'll process 10 sentences at a time). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, test_iter = torchtext.data.BucketIterator.splits(\n",
    "    (pos_train,pos_test), batch_size=10, device=-1,sort_key=lambda x: len(x.txt),repeat = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.5500e+02,  3.3200e+02,  2.5500e+02,  ...,  3.4354e+04,\n",
       "          2.2650e+03,  2.5500e+02],\n",
       "        [ 5.5000e+01,  2.6600e+02,  8.7000e+01,  ...,  8.7000e+01,\n",
       "          2.0000e+00,  1.9800e+02],\n",
       "        [ 2.0000e+00,  4.0000e+01,  2.0000e+02,  ...,  5.0000e+00,\n",
       "          2.4102e+04,  9.0000e+00],\n",
       "        ...,\n",
       "        [ 1.0000e+00,  1.0000e+00,  1.0000e+00,  ...,  1.0000e+00,\n",
       "          1.0000e+00,  1.0000e+00],\n",
       "        [ 1.0000e+00,  1.0000e+00,  1.0000e+00,  ...,  1.0000e+00,\n",
       "          1.0000e+00,  1.0000e+00],\n",
       "        [ 1.0000e+00,  1.0000e+00,  1.0000e+00,  ...,  1.0000e+00,\n",
       "          1.0000e+00,  1.0000e+00]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(train_iter))\n",
    "batch.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also make use of pretrained Word Embeddings trained by Google. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".vector_cache/wiki.simple.vec: 293MB [01:12, 4.06MB/s]                              \n",
      "  0%|          | 0/111052 [00:00<?, ?it/s]Skipping token 111051 with 1-dimensional vector ['300']; likely a header\n",
      "100%|██████████| 111052/111052 [00:14<00:00, 7403.57it/s]\n"
     ]
    }
   ],
   "source": [
    "url = 'https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.simple.vec'\n",
    "TEXT.vocab.load_vectors(vectors=Vectors('wiki.simple.vec', url=url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word embeddings size  torch.Size([224441, 300])\n",
      "Word embedding of 'follows', first 10 dim  tensor([ 0.3925, -0.4770,  0.1754, -0.0845,  0.1396,  0.3722, -0.0878,\n",
      "        -0.2398,  0.0367,  0.2800])\n"
     ]
    }
   ],
   "source": [
    "print(\"Word embeddings size \", TEXT.vocab.vectors.size())\n",
    "print(\"Word embedding of 'follows', first 10 dim \", TEXT.vocab.vectors[TEXT.vocab.stoi['follows']][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay cool. Now that all the preprocessing stuff is done, we can focus on actually building a model. We're going to build a convolutional neural network in pytorch. This involves building a CNN class that inherits nn.Module. We'll implement this paper by Yoon Kim: http://aclweb.org/anthology/D/D14/D14-1181.pdf "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paper does a pretty good job of explaining what a convolutional neural network is, but I can answer questions about the paper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(TEXT.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class customConvNet(nn.Module):\n",
    "    \n",
    "    def __init__(self,input_embeddings, embedding_dim = 300, hidden_size = 100, vocab_size = 235807):\n",
    "        super(customConvNet,self).__init__()  \n",
    "        embedding = nn.Embedding(vocab_size,embedding_dim)\n",
    "        embedding.weight = nn.Parameter(input_embeddings,requires_grad = True)\n",
    "        self.embedding = embedding\n",
    "        self.conv3 = nn.Conv1d(embedding_dim,hidden_size,kernel_size = 3,stride = 1)\n",
    "        self.conv4 = nn.Conv1d(embedding_dim,hidden_size,kernel_size = 4,stride = 1)\n",
    "        self.conv5 = nn.Conv1d(embedding_dim,hidden_size,kernel_size = 5,stride = 1)\n",
    "        self.maxpool = nn.AdaptiveMaxPool1d(1)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.linear = nn.Linear(3*hidden_size,2)\n",
    "        \n",
    "        \n",
    "    def forward(self, input_):\n",
    "        #apply embedding layer\n",
    "        embeds = self.embedding(input_).permute(1,2,0).contiguous()\n",
    "        #apply convolution layers\n",
    "        out1 = F.relu(self.conv3(embeds))\n",
    "        out2 = F.relu(self.conv4(embeds))\n",
    "        out3 = F.relu(self.conv5(embeds))\n",
    "        \n",
    "        #apply max pooling layers\n",
    "        out1 = self.maxpool(out1).squeeze(2)\n",
    "        out2 = self.maxpool(out2).squeeze(2)\n",
    "        out3 = self.maxpool(out3).squeeze(2)\n",
    "        #concatenate the outputs; ending up with a batch_size x 3*hidden_size vector\n",
    "        out = torch.cat((out1,out2,out3),dim = 1)\n",
    "        out = self.dropout(out)\n",
    "        return self.linear(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's initialize an instance of the neural network, and make sure it produces output when we feed in a batch of training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "cn = customConvNet(TEXT.vocab.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0031,  0.3988],\n",
       "        [ 0.2862, -0.1606],\n",
       "        [ 0.0658,  0.2347],\n",
       "        [ 0.1482,  0.3556],\n",
       "        [ 0.1863,  0.0516],\n",
       "        [ 0.1175,  0.2712],\n",
       "        [ 0.2939,  0.1760],\n",
       "        [ 0.0904,  0.3610],\n",
       "        [ 0.0756,  0.0318],\n",
       "        [ 0.2337,  0.3830]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cn(batch.txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, that looks reasonable. The next thing we have to do is write a training loop that will optimize the Convolutional Neural Networks parameters using Stochastic Gradient Descent.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "\n",
    "def model_train(model,train_iter,num_epochs):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=.01)\n",
    "    \n",
    "    for epoch in tqdm_notebook(range(num_epochs),desc = 'Epoch'):\n",
    "        total_loss = 0 \n",
    "        count = 0\n",
    "        model.train()\n",
    "        \n",
    "        for batch in tqdm_notebook(train_iter, desc = 'batch'):\n",
    "            optimizer.zero_grad()\n",
    "            txt = batch.txt\n",
    "            lbl = batch.sentiment\n",
    "            \n",
    "            loss = criterion(model(txt),lbl)\n",
    "            total_loss += loss.data\n",
    "            count += 1\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            torch.nn.utils.clip_grad_norm(model.parameters(), 3)\n",
    "            \n",
    "        print(\"Average NLL: \", (total_loss/count)) \n",
    "        a,b = model_val(model,test_iter)\n",
    "        print(\"Accuracy:\", a)\n",
    "        print(\"Val NLL: \", b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also need a validation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_val(model,val_iter):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    total_loss = 0 \n",
    "    count = 0\n",
    "    correct = 0 \n",
    "    num_examples = 0\n",
    "    model.eval()\n",
    "    \n",
    "    for batch in val_iter:\n",
    "        txt = batch.txt\n",
    "        lbl = batch.sentiment\n",
    "        y_pred = model(txt)\n",
    "        loss = criterion(y_pred,lbl)\n",
    "        total_loss += loss.data\n",
    "        count += 1\n",
    "        \n",
    "        y_pred_max, y_pred_argmax = torch.max(y_pred, 1)\n",
    "        correct += (y_pred_argmax.data == lbl.data).sum()\n",
    "        num_examples += y_pred_argmax.size(0)\n",
    "    model.train()\n",
    "    return(correct/num_examples, total_loss/count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning:** this takes a long time to run.... to speed things up we can run it on a GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40ae6d3c58a0449d97bf33efd587a07c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dc3f5a02e52485485e2cb270f4b4ea6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:22: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average NLL:  tensor(0.6606)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/torchtext/data/field.py:322: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  return Variable(arr, volatile=not train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: tensor(0)\n",
      "Val NLL:  tensor(0.5853)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21326942c63343749e2b622e8108f567"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average NLL:  tensor(0.5381)\n",
      "Accuracy: tensor(0)\n",
      "Val NLL:  tensor(0.4858)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b30049cd6de4053aca0f17323befca3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average NLL:  tensor(0.4672)\n",
      "Accuracy: tensor(0)\n",
      "Val NLL:  tensor(0.4387)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38d476939cca4dbcba680c24e62af663"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average NLL:  tensor(0.4202)\n",
      "Accuracy: tensor(0)\n",
      "Val NLL:  tensor(0.4348)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "316ec22dd48e4ab696da598b3f4ee6a3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average NLL:  tensor(0.3841)\n",
      "Accuracy: tensor(0)\n",
      "Val NLL:  tensor(0.3926)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeb667e5dafd4cf8981dc1789b3abcd0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average NLL:  tensor(0.3512)\n",
      "Accuracy: tensor(0)\n",
      "Val NLL:  tensor(0.3779)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b90c9cb71d0a4f9dada4d59aa72f0f16"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average NLL:  tensor(0.3231)\n",
      "Accuracy: tensor(0)\n",
      "Val NLL:  tensor(0.3609)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b59ae4033aa4d90ac0de4450c439227"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average NLL:  tensor(0.2954)\n",
      "Accuracy: tensor(0)\n",
      "Val NLL:  tensor(0.3528)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6af7f3e6c8b4b22a9fa1b5f261f7e35"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average NLL:  tensor(0.2695)\n",
      "Accuracy: tensor(0)\n",
      "Val NLL:  tensor(0.3523)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "556dde1321dc4966ba3a1afb37bba22e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average NLL:  tensor(0.2438)\n",
      "Accuracy: tensor(0)\n",
      "Val NLL:  tensor(0.3585)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_train(cn,train_iter,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So with a CNN we can get marginal improvements over Naive Bayes and Random Forest. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
